{"version":3,"sources":["../../../src/components/tokenizer/index.ts"],"sourcesContent":["import { createError } from '../../errors.js'\nimport { Stemmer, Tokenizer, DefaultTokenizerConfig } from '../../types.js'\nimport { replaceDiacritics } from './diacritics.js'\nimport { Language, SPLITTERS, SUPPORTED_LANGUAGES } from './languages.js'\nimport { stemmer as english } from './english-stemmer.js'\n\ninterface DefaultTokenizer extends Tokenizer {\n  language: Language\n  stemmer?: Stemmer\n  stemmerSkipProperties: Set<string>\n  stopWords?: string[]\n  allowDuplicates: boolean\n  normalizationCache: Map<string, string>\n  normalizeToken(this: DefaultTokenizer, token: string, prop: string | undefined): string\n}\n\nfunction normalizeToken(this: DefaultTokenizer, prop: string, token: string): string {\n  const key = `${this.language}:${prop}:${token}`\n\n  if (this.normalizationCache.has(key)) {\n    return this.normalizationCache.get(key)!\n  }\n\n  // Remove stopwords if enabled\n  if (this.stopWords?.includes(token)) {\n    this.normalizationCache.set(key, '')\n    return ''\n  }\n\n  // Apply stemming if enabled\n  if (this.stemmer && !this.stemmerSkipProperties.has(prop)) {\n    token = this.stemmer(token)\n  }\n\n  token = replaceDiacritics(token)\n  this.normalizationCache.set(key, token)\n  return token\n}\n\n/* c8 ignore next 10 */\nfunction trim(text: string[]): string[] {\n  while (text[text.length - 1] === '') {\n    text.pop()\n  }\n  while (text[0] === '') {\n    text.shift()\n  }\n  return text\n}\n\nfunction tokenize(this: DefaultTokenizer, input: string, language?: string, prop?: string): string[] {\n  if (language && language !== this.language) {\n    throw createError('LANGUAGE_NOT_SUPPORTED', language)\n  }\n\n  /* c8 ignore next 3 */\n  if (typeof input !== 'string') {\n    return [input]\n  }\n\n  const splitRule = SPLITTERS[this.language]\n  const tokens = input\n    .toLowerCase()\n    .split(splitRule)\n    .map(this.normalizeToken.bind(this, prop ?? ''))\n    .filter(Boolean)\n  const trimTokens = trim(tokens)\n\n  if (!this.allowDuplicates) {\n    return Array.from(new Set(trimTokens))\n  }\n\n  return trimTokens\n}\n\nexport async function createTokenizer(config: DefaultTokenizerConfig = {}): Promise<DefaultTokenizer> {\n  if (!config.language) {\n    config.language = 'english'\n  } else if (!SUPPORTED_LANGUAGES.includes(config.language)) {\n    throw createError('LANGUAGE_NOT_SUPPORTED', config.language)\n  }\n\n  // Handle stemming - It is disabled by default\n  let stemmer: Stemmer | undefined\n\n  if (config.stemming || (config.stemmer && !('stemming' in config))) {\n    if (config.stemmer) {\n      if (typeof config.stemmer !== 'function') {\n        throw createError('INVALID_STEMMER_FUNCTION_TYPE')\n      }\n\n      stemmer = config.stemmer\n    } else {\n      if (config.language === 'english') {\n        stemmer = english\n      } else {\n        throw createError('MISSING_STEMMER', config.language)\n      }\n    }\n  }\n\n  // Handle stopwords\n  let stopWords: string[] | undefined\n\n  if (config.stopWords !== false) {\n    stopWords = []\n\n    if (Array.isArray(config.stopWords)) {\n      stopWords = config.stopWords\n    } else if (typeof config.stopWords === 'function') {\n      stopWords = await config.stopWords(stopWords)\n    } else if (config.stopWords) {\n      throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY')\n    }\n\n    // Make sure stopWords is just an array of strings\n    if (!Array.isArray(stopWords)) {\n      throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY')\n    }\n\n    for (const s of stopWords) {\n      if (typeof s !== 'string') {\n        throw createError('CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY')\n      }\n    }\n  }\n\n  // Create the tokenizer\n  const tokenizer: DefaultTokenizer = {\n    tokenize,\n    language: config.language,\n    stemmer,\n    stemmerSkipProperties: new Set(config.stemmerSkipProperties ? [config.stemmerSkipProperties].flat() : []),\n    stopWords,\n    allowDuplicates: Boolean(config.allowDuplicates),\n    normalizeToken,\n    normalizationCache: new Map(),\n  }\n\n  tokenizer.tokenize = tokenize.bind(tokenizer)\n  tokenizer.normalizeToken = normalizeToken\n\n  return tokenizer\n}\n"],"names":["createError","replaceDiacritics","SPLITTERS","SUPPORTED_LANGUAGES","stemmer","english","normalizeToken","prop","token","key","language","normalizationCache","has","get","stopWords","includes","set","stemmerSkipProperties","trim","text","length","pop","shift","tokenize","input","splitRule","tokens","toLowerCase","split","map","bind","filter","Boolean","trimTokens","allowDuplicates","Array","from","Set","createTokenizer","config","stemming","isArray","s","tokenizer","flat","Map"],"mappings":"AAAA,SAASA,WAAW,QAAQ,kBAAiB;AAE7C,SAASC,iBAAiB,QAAQ,kBAAiB;AACnD,SAAmBC,SAAS,EAAEC,mBAAmB,QAAQ,iBAAgB;AACzE,SAASC,WAAWC,OAAO,QAAQ,uBAAsB;AAYzD,SAASC,eAAuCC,IAAY,EAAEC,KAAa,EAAU;QAQ/E;IAPJ,MAAMC,MAAM,CAAC,EAAE,IAAI,CAACC,QAAQ,CAAC,CAAC,EAAEH,KAAK,CAAC,EAAEC,MAAM,CAAC;IAE/C,IAAI,IAAI,CAACG,kBAAkB,CAACC,GAAG,CAACH,MAAM;QACpC,OAAO,IAAI,CAACE,kBAAkB,CAACE,GAAG,CAACJ;IACrC,CAAC;IAED,8BAA8B;IAC9B,IAAI,CAAA,kBAAA,IAAI,CAACK,SAAS,cAAd,6BAAA,KAAA,IAAA,gBAAgBC,SAASP,QAAQ;QACnC,IAAI,CAACG,kBAAkB,CAACK,GAAG,CAACP,KAAK;QACjC,OAAO;IACT,CAAC;IAED,4BAA4B;IAC5B,IAAI,IAAI,CAACL,OAAO,IAAI,CAAC,IAAI,CAACa,qBAAqB,CAACL,GAAG,CAACL,OAAO;QACzDC,QAAQ,IAAI,CAACJ,OAAO,CAACI;IACvB,CAAC;IAEDA,QAAQP,kBAAkBO;IAC1B,IAAI,CAACG,kBAAkB,CAACK,GAAG,CAACP,KAAKD;IACjC,OAAOA;AACT;AAEA,qBAAqB,GACrB,SAASU,KAAKC,IAAc,EAAY;IACtC,MAAOA,IAAI,CAACA,KAAKC,MAAM,GAAG,EAAE,KAAK,GAAI;QACnCD,KAAKE,GAAG;IACV;IACA,MAAOF,IAAI,CAAC,EAAE,KAAK,GAAI;QACrBA,KAAKG,KAAK;IACZ;IACA,OAAOH;AACT;AAEA,SAASI,SAAiCC,KAAa,EAAEd,QAAiB,EAAEH,IAAa,EAAY;IACnG,IAAIG,YAAYA,aAAa,IAAI,CAACA,QAAQ,EAAE;QAC1C,MAAMV,YAAY,0BAA0BU,UAAS;IACvD,CAAC;IAED,oBAAoB,GACpB,IAAI,OAAOc,UAAU,UAAU;QAC7B,OAAO;YAACA;SAAM;IAChB,CAAC;IAED,MAAMC,YAAYvB,SAAS,CAAC,IAAI,CAACQ,QAAQ,CAAC;IAC1C,MAAMgB,SAASF,MACZG,WAAW,GACXC,KAAK,CAACH,WACNI,GAAG,CAAC,IAAI,CAACvB,cAAc,CAACwB,IAAI,CAAC,IAAI,EAAEvB,QAAQ,KAC3CwB,MAAM,CAACC;IACV,MAAMC,aAAaf,KAAKQ;IAExB,IAAI,CAAC,IAAI,CAACQ,eAAe,EAAE;QACzB,OAAOC,MAAMC,IAAI,CAAC,IAAIC,IAAIJ;IAC5B,CAAC;IAED,OAAOA;AACT;AAEA,OAAO,eAAeK,gBAAgBC,SAAiC,CAAC,CAAC,EAA6B;IACpG,IAAI,CAACA,OAAO7B,QAAQ,EAAE;QACpB6B,OAAO7B,QAAQ,GAAG;IACpB,OAAO,IAAI,CAACP,oBAAoBY,QAAQ,CAACwB,OAAO7B,QAAQ,GAAG;QACzD,MAAMV,YAAY,0BAA0BuC,OAAO7B,QAAQ,EAAC;IAC9D,CAAC;IAED,8CAA8C;IAC9C,IAAIN;IAEJ,IAAImC,OAAOC,QAAQ,IAAKD,OAAOnC,OAAO,IAAI,CAAE,CAAA,cAAcmC,MAAK,GAAK;QAClE,IAAIA,OAAOnC,OAAO,EAAE;YAClB,IAAI,OAAOmC,OAAOnC,OAAO,KAAK,YAAY;gBACxC,MAAMJ,YAAY,iCAAgC;YACpD,CAAC;YAEDI,UAAUmC,OAAOnC,OAAO;QAC1B,OAAO;YACL,IAAImC,OAAO7B,QAAQ,KAAK,WAAW;gBACjCN,UAAUC;YACZ,OAAO;gBACL,MAAML,YAAY,mBAAmBuC,OAAO7B,QAAQ,EAAC;YACvD,CAAC;QACH,CAAC;IACH,CAAC;IAED,mBAAmB;IACnB,IAAII;IAEJ,IAAIyB,OAAOzB,SAAS,KAAK,KAAK,EAAE;QAC9BA,YAAY,EAAE;QAEd,IAAIqB,MAAMM,OAAO,CAACF,OAAOzB,SAAS,GAAG;YACnCA,YAAYyB,OAAOzB,SAAS;QAC9B,OAAO,IAAI,OAAOyB,OAAOzB,SAAS,KAAK,YAAY;YACjDA,YAAY,MAAMyB,OAAOzB,SAAS,CAACA;QACrC,OAAO,IAAIyB,OAAOzB,SAAS,EAAE;YAC3B,MAAMd,YAAY,+CAA8C;QAClE,CAAC;QAED,kDAAkD;QAClD,IAAI,CAACmC,MAAMM,OAAO,CAAC3B,YAAY;YAC7B,MAAMd,YAAY,+CAA8C;QAClE,CAAC;QAED,KAAK,MAAM0C,KAAK5B,UAAW;YACzB,IAAI,OAAO4B,MAAM,UAAU;gBACzB,MAAM1C,YAAY,+CAA8C;YAClE,CAAC;QACH;IACF,CAAC;IAED,uBAAuB;IACvB,MAAM2C,YAA8B;QAClCpB;QACAb,UAAU6B,OAAO7B,QAAQ;QACzBN;QACAa,uBAAuB,IAAIoB,IAAIE,OAAOtB,qBAAqB,GAAG;YAACsB,OAAOtB,qBAAqB;SAAC,CAAC2B,IAAI,KAAK,EAAE;QACxG9B;QACAoB,iBAAiBF,QAAQO,OAAOL,eAAe;QAC/C5B;QACAK,oBAAoB,IAAIkC;IAC1B;IAEAF,UAAUpB,QAAQ,GAAGA,SAASO,IAAI,CAACa;IACnCA,UAAUrC,cAAc,GAAGA;IAE3B,OAAOqC;AACT,CAAC"}